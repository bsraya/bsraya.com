---
title: 'Part 1: Mathematics of Gradient Descent'
type: 'blog'
date: 2022-02-13
published: true
description: 'A mathematical adventure into the gradient descent algorithm'
tags: ['Calculus', 'Optimization', 'Machine Learning']
---

import { Link } from 'gatsby'

This is part 1 of the series on Gradient Descent.

- <Link to="/blog/part-2-gradient-descent">
    Part 2: Gradient Descent
  </Link>
- <Link to="/blog/part-3-batch-gradient-descent">
    Part 3: Batch Gradient Descent
  </Link>
- <Link to="/blog/part-4-stochastic-gradient-descent">
    Part 4: Stochastic Gradient Descent
  </Link>

# Introduction

Remember in the <Link to="/blog/linear-regression">the Linear Regression post</Link>, I mentioned that we can use gradient descent to descrease the MSE of a Linear Regression model.
Let's see how it works at glance. 

# Mathematics of Gradient Descent

Since we want to minimize the MSE value of the Linear Regression model, we want to find the best value for $\beta_0$ and $\beta_0$ so that the regression line that is located as close to most data points as possible. 

From this point, $\beta_0$ will be written as $\Theta_0$ and $\beta_1$ as $\Theta_1$.

Let's express everything we want do in mathematical expressions.

The cost function
$$
    J(\Theta_0, \Theta_1) = \frac{1}{N} \sum_{i=1}^N (f(x) - y_i)^2
$$

The objective function
$$
    \min_{\Theta_0, \Theta_1} J(\Theta_0, \Theta_1)
$$

The update rules
$$
    \Theta_{i} = \Theta_i - \alpha \cdot \frac{\partial}{\partial \Theta_i} J(\Theta_0, \Theta_1)
$$

where

1. $\Theta_{i}$ is the coefficients we want to update
2. $\alpha$ is the learning rate

Bear with me! This is going to involve a lot of Maths, especially Calculus.

Let's simplify the partial derivation in the equation above

$$
    \begin{aligned}
        \frac{\partial}{\partial \Theta_i} J(\Theta_0, \Theta_1) 
        &= \frac{\partial}{\partial \Theta_i} (\frac{1}{N} \sum_{i=1}^N (f(x) - y_i)^2) \\
        &= \frac{1}{N} \frac{\partial}{\partial \Theta_i} \sum_{i=1}^N (f(x) - y_i)^2
    \end{aligned}
$$

Solving the equality above with Power Rule, we then have
$$
    \frac{\partial}{\partial \Theta_i} J(\Theta_0, \Theta_1) = \frac{2}{N} \sum_{i=1}^N (f(x) - y_i) \frac{\partial}{\partial \Theta_i} (f(x) - y_i) 
$$

Since we want to update the $\Theta_0$ and $\Theta_1$ coefficients, we need to find the partial derivative of the cost function with respect to those coefficients.

$$
    \begin{aligned}
        \frac{\partial}{\partial \Theta_0} J(\Theta_0, \Theta_1) 
        &= \frac{2}{N} \sum_{i=1}^N (f(x) - y_i) \frac{\partial}{\partial \Theta_0} (\Theta_0 + \Theta_1 x - y_i) \\
        &= \frac{2}{N} \sum_{i=1}^N (f(x) - y_i)
    \end{aligned}
$$

$$
    \frac{\partial}{\partial \Theta_1} J(\Theta_0, \Theta_1) = \frac{2}{N} \sum_{i=1}^N (f(x) - y_i) x
$$

We can also remove the $2$ from the two equations above by multiplying the cost function, in this case the MSE equation, by $\frac{1}{2}$.
Multiplying the cost function with a scalar will not affect its minimum value.

This new modified cost function is called **One Half Mean Squared Error**.

$$
    J(\Theta_0, \Theta_1) = \frac{1}{2N} \sum_{i=1}^N (\hat{y}_i - y_i)^2
$$

Since the $2$ is removed, 

$$
    \begin{aligned}
        \frac{\partial}{\partial \Theta_0} J(\Theta_0, \Theta_1) = \frac{1}{N} \sum_{i=1}^N (f(x) - y_i) \\
        \frac{\partial}{\partial \Theta_1} J(\Theta_0, \Theta_1) = \frac{1}{N} \sum_{i=1}^N (f(x) - y_i) x 
    \end{aligned}
$$

Pluggin each of the equation above into the update rule with respect to those coefficients, we get
$$
    \begin{aligned}
        \Theta_{0} = \Theta_0 - \alpha \cdot \frac{1}{N} \sum_{i=1}^N (f(x) - y_i) \\
        \Theta_{1} = \Theta_1 - \alpha \cdot \frac{1}{N} \sum_{i=1}^N (f(x) - y_i) x 
    \end{aligned}
$$

The two equations above will help us to approximate the minimum value of the cost function by updating $\Theta_{0}$ and $\Theta_{1}$ over time.

# References