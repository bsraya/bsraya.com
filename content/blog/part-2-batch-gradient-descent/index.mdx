---
title: 'Part 2: Batch Gradient Descent'
type: 'blog'
date: 2022-02-16
published: true
description: 'Linear Regression + Batch Gradient Descent in Python'
tags: ['Machine Learning', 'Optimization', 'Python']
---

import { Link } from 'gatsby';

This is part 2 of the series on Gradient Descent.

- <Link to="/blog/part-1-mathematics-of-gradient-descent">
    Part 1: Mathematics of Gradient Descent
  </Link>
- <Link to="/blog/part-3-mini-batch-gradient-descent">
    Part 3: Mini-Batch Gradient Descent
  </Link>
- <Link to="/blog/part-4-stochastic-gradient-descent">
    Part 4: Stochastic Gradient Descent
  </Link>

# Setting Up The Dataset

In this example, we are going to include the [Iris Dataset](https://archive.ics.uci.edu/ml/datasets/iris) from UCI Machine Learning Repository imported from `scikit-learn`.
There are two features in the dataset that we are going to analyse, namely `sepal_length` and `petal_width` shown in the highlighted lines.

```python {7-8}
from sklearn.datasets import load_iris

iris = load_iris()
features = iris.data
target = iris.target

sepal_length = np.array(features[:, 0])
petal_width = np.array(features[:, 3])

species_names = list()

for i in target:
    if i == 0:
    species_names.append('setosa')
    elif i == 1:
    species_names.append('versicolor')
    else:
    species_names.append('virginica')
```

# Setting Up A Baseline

Before we implement Batch Gradient Descent in Python, we need to set a baseline to compare against our own implementation.
So, we are going to train our dataset into the Linear Regression built-in function made by `scikit-learn`.

First, let's fit our dataset to `python:LinearRegression()` model that we imported from `python:sklearn.linear_model`.

```python
linreg = LinearRegression()

linreg.fit(
    X = sepal_length.reshape(-1,1),
    y = petal_width.reshape(-1,1)
)

print("Intercept: ",linreg.intercept_[0])
# Intercept: -3.200215
print("First coefficient:", linreg.coef_[0][0])
# First coeficient: 0.75291757
```

Once we have the intercept and the coefficient values, let's make a regression line to see if the line is close to most data points.

```python
sns.scatterplot(
    x = sepal_length,
    y = petal_width,
    hue = species_names
)

plt.plot(
    sepal_length,
    linreg.intercept_[0] +
    linreg.coef_[0][0] * features[:, 0],
    color='red'
)
```

![The iris dataset regression line with Scikit](./scikit-lr.png)

Clearly, the line is indeed very close to the most data points and we want to see the MSE of this regression line.

```python
linreg_predictions = linreg.predict(sepal_length.reshape(-1,1))
linreg_mse = mean_squared_error(linreg_predictions, petal_width)
print(f"The MSE is {linreg_mse}")
# The MSE is 0.19101500769427357
```

From the result we got from `python:sklearn`, the best regression line is

$$
    y = -3.200215 + 0.75291757 \cdot x
$$

with MSE value around `0.191`. The equation above is going to be our base line for this experiment to determine how good our own Gradient Descent implementation.

# Implemetation

Remember that in the first part of this series, we have customized the cost function, which is the MSE, simply by multiplying it by $\frac{1}{2}$ and named it **One Half Mean Squared Error**.

$$
    J(\Theta_0, \Theta_1) = \frac{1}{2N} \sum_{i=1}^N (\hat{y}_i - y_i)^2
$$

We have also acquired two equations that are reponsible for updating $\beta_0$ and $\beta_1$. Namely,

$$
    \begin{aligned}
        \beta_0 = \beta_0 - \frac{\alpha}{N} \sum_{i=1}^N (f(x) - y_i) \\
        \beta_1 = \beta_1 - \frac{\alpha}{N} \sum_{i=1}^N (f(x) - y_i) x
    \end{aligned}
$$

Now, let's translate these three equations into Python code.

```python {13-14}
def bgd(x, y, epochs, df, alpha = 0.01):
    intercept, coefficient = 2.0, -7.5

    # first sum_error
    predictions = predict(intercept, coefficient, x)
    sum_error = np.sum((predictions - y) ** 2) / (2 * len(x))
    df.loc[0] = [intercept, coefficient, sum_error]

    for epoch in range(1, epochs):
        predictions = predict(intercept, coefficient, x)
        b0_error = (1/len(x)) * np.sum(predictions - y)
        b1_error = (1/len(x)) * np.sum((predictions - y) * x)
        intercept = intercept - alpha * b0_error
        coefficient = coefficient - alpha * b1_error 
        sum_error = sum_error + np.sum((predictions - y) ** 2) / (2 * len(x))
        df.loc[epoch] = [intercept, coefficient, sum_error]
        sum_error = 0
    return df
```

The highlighted codes is where the parameters update happens.

Once the parameters were updated, we then calculate the cost function for each iteration and save it into the dataframe we created.

```python
bgd_loss = pd.DataFrame(columns=['intercept', 'coefficient', 'sum_error'])
bgd_loss = bgd(sepal_length, petal_width, epochs = 10_000, df = bgd_loss)
```

Below is the figure of the regression lines tend to look like at the 1,000th, the 5,000th, and the 10,000th iterations.

![BGD Loss Function Graph](./iterations.png)

After 10,000 iterations, the MSE value of our own Gradient Descent is $0.195$ which is quite close to our baseline, which is $0.191$.

Combining everything, here is how the regression line changes over time.

![Regression line changes over time](./overall.png)

Let's animate the movement of the regression lines.

![Regression line animation](./gd.gif)

Now, let's see how the movement of the intercept and the coefficient variabels on a contour map.

![The movement of the intercept and the coefficient variabels on a contour map](./contour_map.png)

# Conclusion

Here are some keypoints for Batch Gradient Descent.

1. Batch Gradient Descent only updates the parameters once after considering all the data points.
2. Since the parameters are updated once after considering all the data points, it takes longer time for the algorithm to converge.
3. Not only does Batch Gradient Descent takes a significant amount time to converge due to the large number of data points, but it also takes up a lot of computational resources.
4. Batch Gradient Descent is not the best algorithm for large datasets.
5. Since the gradients we get over time are pretty much the same, then it may stuck in a local minima. Noisy gradients would allow the algorithm to escape local minima.

Since I wanted to present you an easy and succint explanation of Batch Gradient Descent, so I decided to not to include all the codes for the sake of simplicity.
If you want to know how to implement it in more details, please [click here](https://www.kaggle.com/bijonsetyawan/batch-gradient-descent) to check the Python notebook on Kaggle.

# References

1. M. Jack. _3D Gradient Descent in Python_. Source [https://jackmckew.dev/3d-gradient-descent-in-python.html](https://jackmckew.dev/3d-gradient-descent-in-python.html)
2. T. Arseny. _Gradient Descent From Scratch_. Source [https://towardsdatascience.com/gradient-descent-from-scratch-e8b75fa986cc](https://towardsdatascience.com/gradient-descent-from-scratch-e8b75fa986cc)
3. O. Artem. _Stochastic, Batch, and Mini-Batch Gradient Descent_. Source [https://towardsdatascience.com/stochastic-batch-and-mini-batch-gradient-descent-demystified-8b28978f7f5](https://towardsdatascience.com/stochastic-batch-and-mini-batch-gradient-descent-demystified-8b28978f7f5)
4. P. Sushant. _Batch, Mini Batch, and Stochastic Gradient Descent_. Source [https://towardsdatascience.com/batch-mini-batch-stochastic-gradient-descent-7a62ecba642a](https://towardsdatascience.com/batch-mini-batch-stochastic-gradient-descent-7a62ecba642a)
5. Geeksforgeeks. _Difference between Batch Gradient Descent and Stochastic Gradient Descent_. Source [https://www.geeksforgeeks.org/difference-between-batch-gradient-descent-and-stochastic-gradient-descent/](https://www.geeksforgeeks.org/difference-between-batch-gradient-descent-and-stochastic-gradient-descent/)
6. Sweta. _Batch, Mini Batch, and Stochastic Gradient Descent_. Source [https://sweta-nit.medium.com/batch-mini-batch-and-stochastic-gradient-descent-e9bc4cacd461](https://sweta-nit.medium.com/batch-mini-batch-and-stochastic-gradient-descent-e9bc4cacd461)
7. Geeksforgeeks. _ML | Mini-Batch Gradient Descent with Python_. Source [https://www.geeksforgeeks.org/ml-mini-batch-gradient-descent-with-python/](https://www.geeksforgeeks.org/ml-mini-batch-gradient-descent-with-python/)