---
title: 'Part 4: Stochastic Gradient Descent'
type: 'blog'
date: 2022-03-30
published: true
description: 'Linear Regression + another version of Gradient Descent in Python'
tags: ['Machine Learning', 'Optimization', 'Python']
---

import { Link } from 'gatsby';

This is part 3 of the series on Gradient Descent.

- <Link to="/blog/part-1-mathematics-of-gradient-descent">
    Part 1: Mathematics of Gradient Descent
  </Link>
- <Link to="/blog/part-2-batch-gradient-descent">
    Part 2: Batch Gradient Descent
  </Link>
- <Link to="/blog/part-3-mini-batch-gradient-descent">
    Part 3: Mini-Batch Gradient Descent
  </Link>

In this series of post, there two parameters that are crucial in making our model accurate, namely an intercept $\beta_0$ and a coefficient $\beta_1$.

Previously, we have learned that BGD updates them only after it has seen the entire dataset. 
As for MBGD, it only updates them after it has seen a fraction of of the entire dataset.

In this post, we are going to implement another variation of Gradient Descent called Stochastic Gradient Descent, and now I am going to call it SGD throught this post.

# Introduction

SGD is an optimization algorithm that is very similar to BGD and MBGD.
Since we are dealing with Linear Regresison, this algorithm helps us to find the best fit line for our data.
In other words, we want to find the best value for $\beta_0$ and $\beta_1$ so that the line sits right in a position where it's close to most data points.
As the regression line moves toward where most data points sit, the lost function, in this case the Mean Square Error value, will decrease.

Before we start, we should get ourselves familiar with the mathematical part of this algorithm.

# Mathematics of SGD



# Conclusion

If you want to know how to implement it in more details, please [click here](https://www.kaggle.com/bijonsetyawan/stochastic-gradient-descent) to check the Python notebook on Kaggle.

# References

1. O. Artem. _Stochastic, Batch, and Mini-Batch Gradient Descent_. Source [https://towardsdatascience.com/stochastic-batch-and-mini-batch-gradient-descent-demystified-8b28978f7f5](https://towardsdatascience.com/stochastic-batch-and-mini-batch-gradient-descent-demystified-8b28978f7f5)
2. P. Sushant. _Batch, Mini Batch, and Stochastic Gradient Descent_. Source [https://towardsdatascience.com/batch-mini-batch-stochastic-gradient-descent-7a62ecba642a](https://towardsdatascience.com/batch-mini-batch-stochastic-gradient-descent-7a62ecba642a)
3. Geeksforgeeks. _Difference between Batch Gradient Descent and Stochastic Gradient Descent_. Source [https://www.geeksforgeeks.org/difference-between-batch-gradient-descent-and-stochastic-gradient-descent/](https://www.geeksforgeeks.org/difference-between-batch-gradient-descent-and-stochastic-gradient-descent/)
4. Sweta. _Batch, Mini Batch, and Stochastic Gradient Descent_. Source [https://sweta-nit.medium.com/batch-mini-batch-and-stochastic-gradient-descent-e9bc4cacd461](https://sweta-nit.medium.com/batch-mini-batch-and-stochastic-gradient-descent-e9bc4cacd461)
5. R. Sebastian. _Gradient Descent and Stochastic Gradient Descent_. Source [https://rasbt.github.io/mlxtend/user_guide/general_concepts/gradient-optimization/](https://rasbt.github.io/mlxtend/user_guide/general_concepts/gradient-optimization/)
6. Geeksforgeeks. _ML | Mini-Batch Gradient Descent with Python_. Source [https://www.geeksforgeeks.org/ml-mini-batch-gradient-descent-with-python/](https://www.geeksforgeeks.org/ml-mini-batch-gradient-descent-with-python/)